{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import BatchNormalization,Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from IPython.display import SVG,Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting an overview of the dataset\n",
    "import os\n",
    "test=\"E:\\\\Download\\\\Facial_Expression_Recognition_dataset\\\\test\\\\\"\n",
    "train=\"E:\\Download\\Facial_Expression_Recognition_dataset\\\\train\\\\\"\n",
    "#lst=str (os.listdir(\"E:\\\\Download\\\\Facial_Expression_Recognition_dataset\\\\test\\\\\" ))+\"Images\"\n",
    "#lst=list(lst)\n",
    "#print(lst)\n",
    "#for i in lst:\n",
    "    #print(str(len(os.listdir(\"E:\\\\Download\\\\Facial_Expression_Recognition_dataset\\\\test\\\\\"+i)))+\"Images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3995 angry images\n",
      "436 disgust images\n",
      "4097 fear images\n",
      "7215 happy images\n",
      "4965 neutral images\n",
      "4830 sad images\n",
      "3171 surprise images\n"
     ]
    }
   ],
   "source": [
    "for expression in os.listdir(\"E:\\\\Download\\\\Facial_Expression_Recognition_dataset\\\\test\\\\\"):\n",
    "    print(str(len(os.listdir(\"E:\\\\Download\\\\Facial_Expression_Recognition_dataset\\\\train\\\\\"+expression)))+\" \"+expression+' images')\n",
    "#we can serr here we have balanced data set except disgust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras_preprocessing.image.directory_iterator.DirectoryIterator at 0x1350f690c08>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training and Validation batches\n",
    "\n",
    "train_datagen=ImageDataGenerator(horizontal_flip=True)                # Imagedatagenerator performs the standardisation of the \n",
    "                                                                       # image and datagen is its object\n",
    "train_set=train_datagen.flow_from_directory(train,                     # flow_from_directory generates batches\n",
    "                                           target_size=(48,48),\n",
    "                                           batch_size=64,\n",
    "                                           color_mode ='grayscale',\n",
    "                                           class_mode='categorical',\n",
    "                                           shuffle=True\n",
    "                                           )\n",
    "\n",
    "test_datagen=ImageDataGenerator(horizontal_flip=True)                                                                                      \n",
    "test_set=train_datagen.flow_from_directory(test,                     \n",
    "                                           target_size=(48,48),\n",
    "                                           batch_size=64,\n",
    "                                           color_mode='grayscale',\n",
    "                                           class_mode='categorical',\n",
    "                                           shuffle=True\n",
    "                                           )\n",
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making models\n",
    "model=Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding various CNN layers\n",
    "#CNN - 1\n",
    "model.add(Conv2D(512,(3,3),input_shape=(48,48,1)))             # what is passed and why\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))     \n",
    "model.add(BatchNormalization())          # Extra layers for better accuracy\n",
    "model.add(Dropout(0.25))                 # to avoid overfitting\n",
    "\n",
    "#CNN - 2\n",
    "model.add(Conv2D(512,(3,3) ))             \n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))     \n",
    "model.add(BatchNormalization())          \n",
    "model.add(Dropout(0.25))       \n",
    "\n",
    "#CNN - 3\n",
    "\n",
    "model.add(Conv2D(512,(3,3) ))             \n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))     \n",
    "model.add(BatchNormalization())          \n",
    "model.add(Dropout(0.25)) \n",
    "\n",
    "#CNN - 4\n",
    "model.add(Conv2D(512,(3,3) ))             \n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))     \n",
    "model.add(BatchNormalization())          \n",
    "model.add(Dropout(0.25))        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flattening the resultant matrix\n",
    "model.add(Flatten())           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding fully connected dense layers\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "          \n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "          \n",
    "model.add(Dense(7))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling and optimizing the model\n",
    "model.compile(optimizer=Adam(lr=0.0005),loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=5\n",
    "batch_size=64\n",
    "steps_per_epoch=train_set.n//train_set.batch_size\n",
    "validation_steps=test_set.n//test_set.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 448 steps, validate for 112 steps\n",
      "Epoch 1/5\n",
      "448/448 [==============================] - 7663s 17s/step - loss: 2.0272 - accuracy: 0.2740 - val_loss: 1.6828 - val_accuracy: 0.3696\n",
      "Epoch 2/5\n",
      "448/448 [==============================] - 5960s 13s/step - loss: 1.5700 - accuracy: 0.4118 - val_loss: 1.3944 - val_accuracy: 0.4544\n",
      "Epoch 3/5\n",
      "448/448 [==============================] - 10995s 25s/step - loss: 1.3816 - accuracy: 0.4781 - val_loss: 1.2854 - val_accuracy: 0.5078\n",
      "Epoch 4/5\n",
      "448/448 [==============================] - 5922s 13s/step - loss: 1.2640 - accuracy: 0.5217 - val_loss: 1.3042 - val_accuracy: 0.4953\n",
      "Epoch 5/5\n",
      "448/448 [==============================] - 5828s 13s/step - loss: 1.1935 - accuracy: 0.5441 - val_loss: 1.2011 - val_accuracy: 0.5540\n"
     ]
    }
   ],
   "source": [
    "# Fitting / making the model\n",
    "model.fit(\n",
    "    x=train_set,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=epochs,\n",
    "    validation_data=test_set,\n",
    "    validation_steps=validation_steps\n",
    "    )\n",
    "model.save('Emotion_Detection.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Sequential models without an `input_shape` passed to the first layer cannot reload their optimizer state. As a result, your model isstarting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Loading the model\n",
    "from  tensorflow.keras.models import load_model\n",
    "model=load_model('Emotion_detection.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sad\n",
      "Sad\n",
      "Sad\n",
      "Sad\n",
      "Happy\n",
      "Sad\n",
      "Sad\n",
      "Sad\n",
      "Neutral\n",
      "Sad\n",
      "Sad\n",
      "Neutral\n",
      "Sad\n",
      "Neutral\n",
      "Sad\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Sad\n",
      "Neutral\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Neutral\n",
      "Neutral\n",
      "Happy\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Surprise\n",
      "Surprise\n",
      "Surprise\n",
      "Surprise\n",
      "Surprise\n",
      "Surprise\n",
      "Surprise\n",
      "Surprise\n",
      "Surprise\n",
      "Surprise\n",
      "Surprise\n",
      "Surprise\n",
      "Neutral\n",
      "Neutral\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Sad\n",
      "Sad\n",
      "Sad\n",
      "Sad\n",
      "Sad\n",
      "Sad\n",
      "Happy\n",
      "Sad\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Happy\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Sad\n",
      "Sad\n"
     ]
    }
   ],
   "source": [
    "# classification of the labels\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "#test_img=image.load_img(\"E:\\\\Download\\\\Facial_Expression_Recognition_dataset\\\\test\\\\fear\\\\PrivateTest_4002000.jpg\",color_mode = \"grayscale\",target_size=(48,48))\n",
    "\n",
    "# Prediction\n",
    "def predictor(test_img):\n",
    "    \n",
    "    test_img=image.img_to_array(test_img)\n",
    "    test_img=np.expand_dims(test_img,axis=0)\n",
    "    \n",
    "    #Applyig prediction to the model\n",
    "    result = model.predict(test_img)       \n",
    "\n",
    "    #NOTE:Here result is a numpy array which contais the probability of every class: The result class will have the higher probalbilty.\n",
    "\n",
    "    # getting the index of the maximum probability\n",
    "    final_class_index=result.argmax()\n",
    "\n",
    "    #print(final_class_index)\n",
    "\n",
    "    emotions=['Angry','Disgust','Fear','Happy','Neutral','Sad','Surprise']\n",
    "    print(emotions[final_class_index])\n",
    "    return emotions[final_class_index]\n",
    "\n",
    "# Predictions through live camera\n",
    "\n",
    "#Starting the webcam\n",
    "cap=cv2.VideoCapture(0)\n",
    "\n",
    "while(True):\n",
    "    ret,frame=cap.read()\n",
    "    faceCascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')   # ML model to recognise faces\n",
    "    \n",
    "    #Coverting the coloured frame to grayscale image\n",
    "    gray=cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    #Finding the coordinates of  the faces\n",
    "    faces = faceCascade.detectMultiScale(frame, scaleFactor=1.2, minNeighbors=5, minSize=(20, 20))\n",
    "    \n",
    "    for(x,y,w,h) in faces:\n",
    "        #Cropping the face area only\n",
    "        crop=gray[y:y+h,x:x+h]\n",
    "        roi=cv2.resize(crop,(48,48))\n",
    "        path=\"picture.jpg\"     #Saving the cropped face into current folder\n",
    "        cv2.imwrite(path,roi)\n",
    "        \n",
    "        #Calling the predictor function which will return the emotion in the frame\n",
    "        emotion=predictor(roi)\n",
    "        \n",
    "        cv2.rectangle(frame,(x,y),(x+w,y+h),(0,255,0),2)\n",
    "        #cv2.putText(frame,emotion,(x,y),cv2.FONT_HERSHEY_SIMPLEX,1,(255, 0, 0),2)\n",
    "        font = cv2.FONT_HERSHEY_PLAIN\n",
    "        cv2.putText(frame,emotion , (x, y + 30), font, 3,(255,0,0), 3)\n",
    "        \n",
    "    cv2.imshow('frame',frame)\n",
    "    if(cv2.waitKey(1) & 0xFF==ord('q')):\n",
    "       # cv2.imwrite(\"myimage.jpg\",frame)\n",
    "        break\n",
    "        \n",
    "cap.release()  \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
